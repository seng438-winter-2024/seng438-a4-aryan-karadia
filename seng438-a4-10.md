**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group: 10    |
|-----------------|
| Mohamed Ebdalla                |   
| Aryan Karadia              |   
| Raisa Rafi               |   
| Zoraiz Khan             |   

# Introduction


# Analysis of 10 Mutants of the Range class 
## Killed mutants:

1. **getCentralValue()**: It was found that substituting 2.0 with -1.0 changes the calculation of the central value, which was caught by the tests checking this method.
2. **intersects()**: It was noticed that removing a conditional or negating a return statement alters the logic of intersection, which was detected by the tests for this method.
3. **constrain()**: It was observed that changing the `!contains(value)` check to a less than or equal check modifies the constraint logic, which was caught by the tests checking the constraints.
4. **combine()**: It was discovered that removing the null check for `range1` could lead to a NullPointerException, which was caught by the tests checking the combination of ranges.
5. **combineIgnoringNaN()**: It was identified that removing the call to `java/lang/Double::isNaN` changes the logic of the combination operation, which was caught by the tests checking this method.

## Surviving mutants:

1. **getLength()**: It was found that negating the `lower` field or replacing subtraction with addition changes the length calculation, which seems to have slipped through the tests.
2. **intersects()**: It was noticed that changing the `(b0 <= this.lower)` conditional boundary alters the intersection logic, which seems to have not been covered by the tests.
3. **constrain()**: It was observed that changing the inequality checks could modify the constraint logic, which seems to have not been covered by the tests.
4. **shift()**: It was discovered that negating the conditional at line 4 changes the logic of the shift operation, which seems to have not been covered by the tests.
5. **equals()**: It was identified that changing the equality check to less than or equal at line 7 alters the equality logic, which seems to have not been covered by the tests.


# Report all the statistics and the mutation score for each test class

## Test Suite from assignment 3

Our base test suite is shown to have covereage of **94%** Line Coverage, **82%** Mutation Coverage, and **89%** Test Strength for **DataUtilities.java**.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/105018373/49e8480a-ab79-4d33-9b91-ca4599521e48)

Moreover, it has a 86% Line coverage, 60% Mutation Coverage, and 69% Test Strength for Range.java.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/105018373/43724afb-11b3-4267-a735-f79988b7b01d)

## Test Suite with added Mutation Coverage Tests

# Analysis drawn on the effectiveness of each of the test classes

# A discussion on the effect of equivalent mutants on mutation score accuracy
Since equivalent mutants cannot be killed by the test suite, they will negitvely impact the score accuracy. They directly lead to a higher amount of surviving mutants thus it is important for them to be found and ignored when measuring accuracy. 
## Automatically Finding Equivalent Mutants
Since detecting equivalent mutants is a challenging task, a potential approach for automatic equivalent mutant detection can be code coverage analysis. Code coverage analysis is a method of detecting equivalent mutants by comparing the code coverage of the original test suite with that of the test suite when run against the mutant. The benefit of this approach is that it can be automated and can help identify equivalent mutants without manual intervention. However, the disadvantage is that it may not be accurate as different code paths can result in the same coverage. This method assumes that the test suite is comprehensive and tests all aspects of the program's behavior.

# A discussion of what could have been done to improve the mutation score of the test suites

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

# Explain your SELENUIM test case design process

We wanted to design test cases which verify the inputs a user is giving to a GUI are getting read and handeled properly. Our goal was to test the most common features a user would use when interacting with the website. For each functionality we thought was important, such as logging in, creating accuont, buying a product, etc. we wanted to test the response of the GUI given invalid and valid inputs.

## Test Case #1: AddMaxAmountOfProductToCart

For this test case I wanted to test the GUI's response to an invalid input in the "quantity" field when buying a product. The path the script follows is:

1. Search for item
2. Verify products show up
3. Choose option
4. Change quantity to 100
5. Try and add to cart
6. Assert error message shows up and does not add to cart

## Test Case #2: LogInWithInvalidUser

This simple test cases tests whether or not the GUI is updated properly when an invalid email address is inputted when trying to log in. The path of the script is:

1. Click `Sign In` button
2. Enter invalid email id in to `userid` field
3. Attempt to sign in by clicking `Continue` button
4. Assert an error message pops up and GUI does not continue 

# Explain the use of assertions and checkpoints

# how did you test each functionaity with different test data

# Discuss advantages and disadvantages of Selenium vs. Sikulix

# How the team work/effort was divided and managed

In our team, we divided the work as follows:

## PART 1 (Mutation Test Writing)
- Each team member wrote tests to improve the **mutation score** by at least 2.5% for both the `Range` and `DataUtilities` class.
- We answered the lab report questions related to our written test cases.

## PART 2 (Selenium Automation):
- We automated at least two different functionalities of the application under test.
- Detailed documentation was created for our automated scripts.
- We also addressed lab report questions related to our designed testing scripts, mainly on the design process for each test case.

# Difficulties encountered, challenges overcome, and lessons learned

# Comments/feedback on the lab itself
