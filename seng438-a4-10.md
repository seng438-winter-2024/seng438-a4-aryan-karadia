**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 â€“ Mutation Testing and Web app testing**

| Group: 10    |
|-----------------|
| Mohamed Ebdalla                |   
| Aryan Karadia              |   
| Raisa Rafi               |   
| Zoraiz Khan             |   

# Introduction


# Analysis of 10 Mutants of the Range class 
## Killed mutants:

1. **getCentralValue()**: It was found that substituting 2.0 with -1.0 changes the calculation of the central value, which was caught by the tests checking this method.
2. **intersects()**: It was noticed that removing a conditional or negating a return statement alters the logic of intersection, which was detected by the tests for this method.
3. **constrain()**: It was observed that changing the `!contains(value)` check to a less than or equal check modifies the constraint logic, which was caught by the tests checking the constraints.
4. **combine()**: It was discovered that removing the null check for `range1` could lead to a NullPointerException, which was caught by the tests checking the combination of ranges.
5. **combineIgnoringNaN()**: It was identified that removing the call to `java/lang/Double::isNaN` changes the logic of the combination operation, which was caught by the tests checking this method.

## Surviving mutants:

1. **getLength()**: It was found that negating the `lower` field or replacing subtraction with addition changes the length calculation, which seems to have slipped through the tests.
2. **intersects()**: It was noticed that changing the `(b0 <= this.lower)` conditional boundary alters the intersection logic, which seems to have not been covered by the tests.
3. **constrain()**: It was observed that changing the inequality checks could modify the constraint logic, which seems to have not been covered by the tests.
4. **shift()**: It was discovered that negating the conditional at line 4 changes the logic of the shift operation, which seems to have not been covered by the tests.
5. **equals()**: It was identified that changing the equality check to less than or equal at line 7 alters the equality logic, which seems to have not been covered by the tests.


# Report all the statistics and the mutation score for each test class

## Test Suite from assignment 3

Our base test suite is shown to have covereage of **94%** Line Coverage, **82%** Mutation Coverage, and **89%** Test Strength for **DataUtilities.java**.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/105018373/49e8480a-ab79-4d33-9b91-ca4599521e48)

Moreover, it has a 86% Line coverage, 60% Mutation Coverage, and 69% Test Strength for Range.java.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/105018373/43724afb-11b3-4267-a735-f79988b7b01d)

## Test Suite with added Mutation Coverage Tests
After adding some more coverage tests, we got the DataUtilities Mutation score to increase by 9%. We could not get the last 1% because our test suite already had relatively high mutation coverage.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/114327156/63efb39d-fd66-4ffc-8cb1-3224595c294c)

For the Range class, we got the mutation score to increase by 11% after adding additional coverage tests.
![image](https://github.com/seng438-winter-2024/seng438-a4-aryan-karadia/assets/114327156/44df1251-f759-49e7-a5bc-0c730ad36ea5)


# Analysis drawn on the effectiveness of each of the test classes

# A discussion on the effect of equivalent mutants on mutation score accuracy
Since equivalent mutants cannot be killed by the test suite, they will negitvely impact the score accuracy. They directly lead to a higher amount of surviving mutants thus it is important for them to be found and ignored when measuring accuracy. 
## Automatically Finding Equivalent Mutants
Since detecting equivalent mutants is a challenging task, a potential approach for automatic equivalent mutant detection can be code coverage analysis. Code coverage analysis is a method of detecting equivalent mutants by comparing the code coverage of the original test suite with that of the test suite when run against the mutant. The benefit of this approach is that it can be automated and can help identify equivalent mutants without manual intervention. However, the disadvantage is that it may not be accurate as different code paths can result in the same coverage. This method assumes that the test suite is comprehensive and tests all aspects of the program's behavior.

# A discussion of what could have been done to improve the mutation score of the test suites

One thing that could have been done to improve the mutation score is to ensure that the tests cover most of the code. By increasing coverage, there's a higher chance of detecting mutations. It is also important to focus on strengthening tests in areas where mutations are frequently surviving. Another way to improve the mutation score is to create diverse tests that take various scenarios and edge cases into consideration.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Mutation testing assesses the quality and effectiveness of a test suite, where mutations are placed into the source code to see if they will be detected and killed. We use it to determine where tests are weak, in order to provide more confidence in our tests. 

Advantages:
1. Quality Improvement
2. Fault Localization
3. Detection of Redundant Tests
4. Effectiveness Assessment

Disadvantages:
1. Computentionally expensive
2. Unmeaningful mutations, lead to false positives
3. Overhead in Maintenance
4. Hard to Interpret





# Explain your SELENUIM test case design process

We wanted to design test cases which verify the inputs a user is giving to a GUI are getting read and handeled properly. Our goal was to test the most common features a user would use when interacting with the website. For each functionality we thought was important, such as logging in, creating accuont, buying a product, etc. we wanted to test the response of the GUI given invalid and valid inputs.

## Test Case #1: AddMaxAmountOfProductToCart

For this test case I wanted to test the GUI's response to an invalid input in the "quantity" field when buying a product. The path the script follows is:

1. Search for item
2. Verify products show up
3. Choose option
4. Change quantity to 100
5. Try and add to cart
6. Assert error message shows up and does not add to cart

## Test Case #2: LogInWithInvalidUser

This simple test cases tests whether or not the GUI is updated properly when an invalid email address is inputted when trying to log in. The path of the script is:

1. Click `Sign In` button
2. Enter invalid email id in to `userid` field
3. Attempt to sign in by clicking `Continue` button
4. Assert an error message pops up and GUI does not continue

## Test Case #3: Add and Remove Item from Cart

For this test case we are testing the GUI's response to adding an item to cart and then removing it. The path the script follows is:

1. Search for item
2. Verify products show up
3. Choose the item
4. Add the item to cart
5. Click "Remove" button
7. Assert that the item is removed from the cart

## Test Case #4: Enter Item Amount as 0

For this test case we are testing the GUI's response to adding an item to cart and then removing it. The path the script follows is:

1. Search for item
2. Verify products show up
3. Choose the item
4. Change quantity to 0
5. Try and add it to cart
6. Assert error message shows up and does not add to cart

## Test Case #5: JeweleryAuctionAscendingOrder

This simple test cases tests whether or not the GUI is updated properly when the request to view the soonest ending auction for jewlery is made:

1. Click `Shop by Category` button
2. Click `Jelwery and Watches` button
3. Click `Best Match` dropdown menu
4. Click `Time: Ending Soonest`
5. Assert auction times are listed in ascending order from soonest to latest

## Test Case #6: FilterValidItemSearchTextAndPrice

This simple test cases tests whether or not the GUI is updated properly when the request to view the soonest ending auction for jewlery is made:

1. Click `Advanced` button
2. Enter valid item in `Enter keywords or item number` box
3. Change minimum price to $100
4. Change maximum price to $1000
5. Assert all items are related to text item searched for and within the entered price range

## Test Case #7: VerifySearchFunctionality

This simple test case tests whether or not the GUI can successfully search for products on the website:

1. Click on the search submission form
2. Enter a search query for `iphone 15`
3. Click `Search` button
4. Assert search result loaded for search query

## Test Case #8: ContactUsFormSubmission

This simple test cases tests whether or not the GUI can submit customize queries through the contact us form:

1. Click `Help and Contact` button
2. Click `Search eBay Help...` submission form
3. Enter `item not delivered` in submission form
4. Click `Search` button
5. Assert search result loaded for search query

# Explain the use of assertions and checkpoints

Assertions and checkpoints are verification methods which ensure the behaviour of the application is expected. An assertion is a statement which can be used to validate whether certain conditions are true or false. For example, you can assert that a certain element is present on a web page, and if that condition is met the test case will pass. Checkpoints are used to validate the state of the application at a specific point, so they will check whether an element is present or not. Thus, checkpoints will be frequently used during the execution of a test to ensure that the application is in the desired state before proceeding. Assertions and checkpoints are important in automation testing to verify expected behaviors, validate application states and ensure the accuracy of test results. 

# how did you test each functionaity with different test data

To test each functionality with different test data, we identified specific scenarios for each functionality, such as adding products to the cart or logging in with invalid credentials. Then, we created test cases with different sets of test data, covering valid inputs, invalid inputs, boundary values, and edge cases. We then used Selenium automation to execute the test cases, providing input data and verifying the application's response against expected outcomes using assertions.

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Advantages of Selenium:
- Supports a variety of browsers like Chrome, Firefox, and Safari, which makes it easy to test web applications across different environments
- Supports C#, Java, Perl, PHP, Python, Ruby, Javascript
- Integrates with other tools like TestNG, JUnit, and Maven, alllowing for test automation
- Easy to record and play
- Easy to debug and set breakpoints

Disadvantages of Selenium:
- mostly designed for web-applications and lacks support for automating desktop or non-UI based applications
- can sometimes struggle with dynamic content or changes in the UI layout
- Selenium tests are require regular maintenance since web applications often evolve
- does not have built-in support for image-based automation

Advantages of Sikulix:
- allows image-based automation, enabling testing of applications where traditional automation tools fail
- works across diffferent platforms like Windows, macOS and Linux
- can interact with dynamically changing elements
 
Disadvantages of Sikulix:
- image-based automation can be slow especially when dealing with large and complex applications
- since it primarily relies on image recognition, it may struggle with text recognition
- may require frequent maintenance as changes in UI elements can affect the reliability of image-based recognition

# How the team work/effort was divided and managed

In our team, we divided the work as follows:

## PART 1 (Mutation Test Writing)
- Each team member wrote tests to improve the **mutation score** by at least 2.5% for both the `Range` and `DataUtilities` class.
- We answered the lab report questions related to our written test cases.

## PART 2 (Selenium Automation):
- We automated at least two different functionalities of the application under test.
- Detailed documentation was created for our automated scripts.
- We also addressed lab report questions related to our designed testing scripts, mainly on the design process for each test case.

# Difficulties encountered, challenges overcome, and lessons learned

The difficulties of this lab mainly was setting up the environment so the PIT would run properly. Another challenge in this lab was trying to write tests to improve mutation coverage when java test coverage was already at 100%.  

# Comments/feedback on the lab itself
The lab provided valuable instructions for setting up PiTest in Eclipse. It also offered practical insights on using Selenium for web application testing.

